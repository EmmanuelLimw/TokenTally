# ‚öôÔ∏è TokenTally
### Estimate Your LLM's Token Toll Across Various Platforms and Configurations

üéØThe goal is to be able to calculate the **minimum GPU requirements** for **Training**(Fine Tuning and Continued Pre Training) and **Inference** for any LLM along with Comparison to Self-Host these models across different GPU Cloud Platforms and Optimizations. Eventually to Calculate cost/token for every possible combinations of Model, Platform and Optimizations!

![cutie](https://github.com/adarshxs/TokenTally/assets/114558126/0f584e00-5bf8-4763-a885-8ca5a7e87ee9)



All props to https://github.com/cloud-gpus/cloud-gpus.github.io from where I stole the list of available GPUs and their pricing ;)
